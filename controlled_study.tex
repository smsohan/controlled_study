
\documentclass[conference]{IEEEtran}


\ifCLASSINFOpdf
  \usepackage{listings}
  \usepackage[pdftex]{graphicx}
  \usepackage{url}

\else
\fi
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
\title{How Important are Usage Examples in REST API Documentation?}

\author{\IEEEauthorblockN{S M Sohan, Frank Maurer\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}
Dept. of Computer Science\\
University of Calgary\\
Canada\\
\{smsohan, frank.maurer\}@ucalgary.ca}
\and
\IEEEauthorblockN{Craig Anslow}
\IEEEauthorblockA{School of Eng. and Computer Science\\
Victoria University of Wellington\\
New Zealand\\
craig@ecs.vuw.ac.nz}
\and
\IEEEauthorblockN{Martin Robillard}
\IEEEauthorblockA{
School of Computer Science\\
McGill University\\
Canada\\
martin@cs.mcgill.ca}
}

\maketitle

\begin{abstract}

[Brain-dump mode]

What is the the problem? Effort and cost associated to REST API documentation with usage examples.
Why is this a problem? Cost vs. benefit of usage examples is not known. What to include in the examples?
What have we done? Performed an empirical study with 20 professional software engineers with REST API experience to understand how their productivity and types of problems faced changes with respect to usage examples.
What have we learned? (Hoping) examples help developer perform task with fewer problems with greater success, with fewer errors, and faster in terms of time.
What does it mean? (expected) API developers should include usage examples in REST API documentation that include specific examples based on the problems faced.


\end{abstract}


\IEEEpeerreviewmaketitle

\section{Introduction}
\section{Methodolody}
\subsection{Research Goals}

This research is aimed at answering the following research questions:

\begin{itemize}
  \item \textbf{RQ}. What obstacles API client developers face when using a REST API documentation that lacks usage examples?
\end{itemize}

\subsection{Requirements} % (fold)

To answer the aforementioned research questions, the study has the following requirements:

\begin{itemize}
  \item \textbf{R1. Representative API.} We had to choose an existing REST API that is currently used by API client developers. Selecting a mature REST API for this study reduces the possibility of an underdeveloped solution that may be found if a new or a seldom used API is selected. A familiar domain needed to be selected so that participants are able to relate to the API features without requiring upfront training. In addition to selecting the API, we had to select tasks that are related to the core features provided by the API to represent reality.
  \item \textbf{R2. Open source.} To be able to understand the impact of usage examples, we needed to select an open-source API where we can add new examples to the documentation for performing this study. For proprietary APIs, it won't be possible to retrofit usage examples on existing API documentation without separating the examples from the documentation.
  \item \textbf{R3. Time bound.} The total time spent by each participant is required to be time bound to measure productivity. As a result, the study needs to be setup such that participants are able to focus on performing the tasks minimizing any overhead.
  \item \textbf{R4. Experienced developers.} Developers with prior experience on REST APIs need to be recruited as study participants to perform the study within a limited amount of time and in a realistic setup. Furthermore, to reduce a learning bias, only participants with no prior experince of using the WordPress REST API V2 are accepted for this study.
\end{itemize}


\subsection{Study API} % (fold)

We selected the WordPress REST API V2 for this study. WordPress is a blog-like open-source (R2) framework used by over 409 million people to visit 23.6 billion pages each month. The API allows programmatic access to list, create, update, and delete WordPress data such as blog posts, comments, users, images, tags, etc.

The WordPress REST API V2 has been published and maintained since May 2015. Before January 2017, the WordPress REST API V2 was distributed as a plug-in where WordPress users could optionally install the API component. The following statistics are for the plug-in install numbers between May 2015 and October 2016:

\begin{itemize}
  \item Total installs: aprox. 248K installs of the plug-in.
  \item Average daily installs: approx. 500.
\end{itemize}

Starting January 2017, the WordPress REST API V2 is no longer required to be installed as a separate plug-in since it's pre-bundled with every WordPress install. As per the code repository on GitHub, there are a total of 99 and 46 contributors that had at least one commit to the code repository behind the API and and it's documentation, respectively. These properties satisfy R1, our requirements for using a representative API.

By selecting an open-source project we are able to access the source-code to inspect the implementation and documentation technique of the WordPress REST API. The API implements a self-documenting feature where API developers expose API endpoints over HTTP OPTIONS verb to explain the API elements. To implement this feature, the API developers describe the API elements in the code. For example:


\lstset{basicstyle=\footnotesize}
\begin{lstlisting}[language=php,breaklines=true,showspaces=false,showstringspaces=false,numbers=left,xleftmargin=2em,caption={Example of self-documenting API Code},label=wordpress_code]
public function get_item_schema() {
  $schema = array(
    '$schema'    => 'http://json-schema.org/draft-04/schema#',
    'title'      => $this->post_type,
    'type'       => 'object',
    /*
     * Base properties for every Post.
     */
    'properties' => array(
        'date'            => array(
            'description' => __( "The date the object was published, in the site's timezone." ),
            'type'        => 'string',
            'format'      => 'date-time',
            'context'     => array( 'view', 'edit', 'embed' ),
        ),...
\end{lstlisting}

On Listing \ref{wordpress_code}, line 4 specifies that this is a schema definition for the API element $Post$. Then, on Line 10, it defines $date$, one of the properties of $Post$, followed by a human readable description and and type information. Then, on line 14, the context of this property is mentioned as $view, edit, embed$, meaning that this property will be returned when the $Post$ object is returned, embedded, or can be used as an input for editing.

This self-documenting feature is leveraged to generate and publish the official API documentation as an HTML website. Figure \ref{fig:schema} shows a screenshot of the published documentation for the $date$ attribute of the $Post$ API element\footnote{\url{http://v2.wp-api.org/reference/posts/}}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{schema.png}
\end{center}
\caption{Screenshot showing auto-generated documentation for Post/date}
\label{fig:schema}
\end{figure}

In addition to the auto-generated documentation of the the schema and API actions, custom content is added by the contributors to the API documentation project to provide prosaic overviews and usage examples.



\subsection{Study Design} % (fold)
\subsubsection{Tasks} % (fold)

The participants are requested to perform a total of six tasks using the API including one practice task. All of the tasks are related to a single API element, $Post$. The tasks get progressively difficult, and all but the last task can be performed independently of each other. Participants are requested to limit the total time on the study to a maximum of one hour. Participants are encouraged to proceed to the next task when they are either satisfied about or stuck on the right answer.

To use a REST API, the API client developers need to work with the following four inputs over HTTP:

\begin{itemize}
  \item I1. Request method
  \item I2. Request URL
  \item I3. Request headers
  \item I4. Request body
\end{itemize}

To verify the response of an API call, the API client developers can use HTTP response headers and/or HTTP response body. To perform the tasks, the participants are required to use one or more of the inputs I1-4. In the following paragraphs, we describe each task with it's description and the intended test scenario against the aforementioned API input and output information. For each of the tasks, the participants are required to use the same WordPress REST API and its documentation.

\textbf{T1: ListAllPostsTask}. We ask the participants to use the WordPress REST API following its documentation to get a list of the blog posts from a live WordPress site. This is the practice task, and the inputs to answer this task are pre-filled for the participants. It allows the participants to understand the tools used for this study as well as get familiarity with the Post API. The answer for this task makes use of I1 and I2.

\textbf{T2: FilterPostsByAuthorTask.} The participants are asked to use the API to filter the list of posts by an author given the author's user name. To answer this correctly, the participants are required to first make an API call to get the numeric ID of the author given the string based user name. Then, the ID needs to be used on the Post API to filter posts by the author. This task allows us to understand the impact of usage examples on API client developers when multiple API calls need to be made to perform a task using the API. Inputs I1-2 are required to complete this task successfully.

\textbf{T3: ExcludePostsByIdsTask.} We ask the participants to use the API to get a list of all posts excluding posts with IDs 1 and 4. Participants need to use the inputs I1-2, and use a desired format on I2 to pass an array of IDs as a parameter. This task allows us to understand how API client developers identify the format for using an array within the URL with respect to the usage examples in the API documentation.

\textbf{T4: FindTotalPostsTasks.} This task requires the participants to use the API to find a total number of posts. Participants need to use the inputs I1-2 and inspect the HTTP response headers to successfully complete this task. This task allows us to understand how API client developer productivity is affected with respect to missing examples about HTTP response headers in the API documentation.

\textbf{T5: PublishPostTask.} We ask the participants to use the API to publish a blog post with a specific title, content, and a published date. To successfully complete this task, the participants are required to use all four input types and inspect both the HTTP response header and the response body. Additionally, the participants are required to use a specific date format that the API accepts as a valid format for date specification. Answers to this task allows us to study API client developer productivity with respect to the usage examples lacking details about the inputs I3-4.

\textbf{T6: UpdatePostTask.} We ask the participants to use the API to update a blog post that they published in T5 with a new excerpt. Similar to T5, this task requires the use of inputs I1-4, but with different values for the inputs. This task allows us to understand API client developer productivity on inter-dependent tasks with respect to usage examples.

To summarize, the tasks allow us to understand how REST API client developers approach API tasks of different complexity levels involving various input types and available output information with respect to the usage examples in the API documentation.

\subsubsection{Participant Selection} % (fold)

To satisfy the requirement of developers with REST API experience (R3), we have used the following criteria for recruiting the study participants:

\begin{itemize}
  \item Currently working as a software engineer.
  \item At least 1 year of industry experience as a software engineer.
  \item At least 1 year of industry experience with REST APIs.
  \item No prior experience with WordPress REST API.
\end{itemize}

Participants were recruited through online announcements posted on Twitter, Facebook, and software developer focused mailing lists.

\subsubsection{Process}

Two pilot studies were performed to evaluate and understand an effective process for performing this study. The first pilot study involved four participants that were invited to join the first author on this paper in-person or using a video conferencing software. The study involved tasks using two APIs, the WordPress REST API V2 and the GMail REST API. Each participant was given one of the two APIs and a set of tasks to complete using the API within an hour. Participants were given an online answer form to record the answers to the tasks. The primary findings from this pilot are as follows: 1) asking participants to use an API to perform the tasks required significant overhead time for them to setup a development environment with the proper API credentials, 2) the intermediate trial attempts of using the API are potentially more valuable than the final answer as it allows us to understand API client developer information needs that may are not answered by the documentation, 3) the number of tasks for the study had to be reduced so the participants could complete the tasks within the one hour limit, and 4) for GMail API, participants used up a large portion of their time on setting up their API credentials that requires understanding of OAuth.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{api_explorer.png}
\end{center}
\caption{Screenshot of the web-based REST API explorer}
\label{fig:api_explorer}
\end{figure*}

To overcome the shortcomings found from the first pilot study, we decided to develop a web-based REST API explorer as shown on Figure \ref{fig:api_explorer} that allows participants to use their browser to make the API calls without setting up any development environment. The web-based API explorer only requires the inputs I1-4, and displays the HTTP response headers and body on the click of a button. Thus the participants could focus on using the right input and verifying the output without having to write any code. The web-base REST API explorer also allowed us to automatically collect all the trial API calls that the participants make for each API task. A second pilot study involving seven new participants was performed to understand the features of the web-based REST API explorer in practice and to improve the user interface based on feedback from the participants. Participants completed the study on their own without having to meet in-person or over video conferencing. Only the WordPress REST API was used to focus on REST APIs without the required learning curve associated with OAuth. We found encouraging results from this pilot study as the collected data showed patterns of mistakes that API client developers make that can be reduced by adding usage examples in the API documentation.


\begin{figure*}[t]
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{original.png}
\end{center}
\caption{Screenshot of the original WordPress REST API documentation}
\label{fig:original}
\end{figure*}

\begin{figure*}[tb]
\begin{center}
\includegraphics[width=\linewidth,keepaspectratio]{forked.png}
\end{center}
\caption{Screenshot of the forked WordPress REST API documentation with a usage example}
\label{fig:forked}
\end{figure*}


Based on the lessons learned from the pilot studies, we designed the actual study process as follows: the original WordPress REST API documentation was forked and a total of 3 API usage examples were added to show listing of blog posts with query parameters for filtering, a request to create a blog post and a request to update a blog post. Figure \ref{fig:original} shows a screenshot of the original API documentation related to T4 where the API client developers are provided with a reference table describing the different properties that can be used to create a $Post$ object\footnote{\url{http://v2.wp-api.org/reference/posts/}}. Figure \ref{fig:forked} shows a screenshot of the forked API documentation with a cURL based usage example. cURL is used because it's used by elsewhere in the original API documentation. In the forked API documentation, the example shows one possible API call with realistic values for the data that is described in the reference table and associated API response headers and body.

Participants were divided into two groups, G1 and G2. G1 participants were provided with a link to the official API documentation on the web-based API explorer, and G2 participants were provided with a link to the forked API documentation with usage examples. The web-based API explorer allocated more participants to G1 compared to G2 because we wanted to better understand the impact of the lack of usage examples on API client developer productivity. However, each individual participant was randomly assigned to a group by the web-based API explorer. All participants were given the same set of API tasks and were requested to limit their participation time to a maximum of one hour. No task specific time limit was imposed because we wanted participants to spend sufficient time on each task without forcing them to move the next one. Participants were allowed to access the internet and external resources alongside the provided API documentation to perform the tasks as they'd normally use on a typical work day.

\subsubsection{Data Collection} % (fold)
The data collected by the web-based REST API explorer for each participant is exported into a text file as the raw data artifact. For each API task and each participant that attempted the task, the exported text file contains the request inputs I1-4 and associated API response headers and body for each API call made by the participant. For each participant, we recorded all trial API calls with timestamps. Additionally, the demographic information, experience rating and the free-form feedback for each participant is also included in the artifact.


\subsubsection{Data Analysis}
The same data artifact is used to analyze both quantitatively and qualitatively to answer the aforementioned research question.We analyzed the data artifact qualitatively to understand the obstacles that REST API client developers face when using an API documentation that lacks usage examples. The data artifact exported from the web-based REST API explorer included a table with raw data. For each row, the table contains the following columns: API task, participant identifier, timestamp, API trial number, I1-4, response headers and body. It allowed us to observe and categorize the different input values that the participants used to perform the given API tasks. The values for I1-4 used by the participants were manually coded to group the participant responses into categories. The categories help us determine the types of information that API client developers need in the API documentation to perform API tasks successfully. The analysis started with an empty set of codes and new codes were introduced to describe scenarios that didn't fall under  codes that were already applied. The first author on this paper applied the codes on the raw data artifact and the co-authors verified the codes randomly. To resolve disagreements, the authors discussed and updated the codes accordingly. The codes were then further categorized to higher level grouping to represent common problems faced by the study participants for each group.

For each API task attempted by each participant, we annotated the artifact with one of the following labels: successful, partially successful, and unsuccessful. Task participations are marked successful when I1-4 matches the required values for performing the given task. If a participant is able to use the correct I1-4 for one of the two tasks required to complete a single task (T2), we marked it as a partially successful. Otherwise, it's marked as unsuccessful. Based on these annotations, the answers for RQ1 was computed using the following:


success rate of group = (total number of successful tasks by group) / (number of participants in group * number of API tasks)

API calls per success of group = (number of API calls annotated as successful) / (total number of trial API calls made by group)

average time taken for success = (number of successful API tasks by group) / (total time spent by all participants in the group)


\section{Results}

\subsection{Qualitative Analysis Results} % (fold)

  Observation and Implications
    For each task, describe the results. Provide examples of failed attempts with counts, and relate to the documentation sources to share an observation. Imply what needs to be done.


\subsection{Quantitative Analysis Results} % (fold)

A graph showing the difference in productivity metrics between G1 and G2.
Implications


% subsection rq1 (end)



\section{Threats to Validity}
  API Selection
  Task Selection
  Participant Selection
  Experiment Design

\section{Related Work}

\subsubsection{API Learning Obstacles}
  Researchers have published several papers on the topic of API learning obstacles. Robillard et al. used a mixed approach of surveys and in-person interviews with professional software engineers to understand the API learning obstacles \cite{Robillard_what_makes} \cite{Robillard_a_field_study}. They found that API documentation was related to one of the most severe obstacles for API learnabilty and recommended considering five factors for API documentation as follows: documentation of intent, code examples, matching APIs with scenarios, penetrability of the API, and format and presentation. Gias et al. surveyed 323 professional software engineers to understand the factors that fail a API documentation \cite{g_uddin}. They identified ambiguity, incompleteness, and incorrectness as the three severest problems that lead API documentations to fail to answer the information needs of developers. Duala-Ekoko et al. performed a study with twenty programmers to understand the types of questions that developers have while using an unfamiliar API \cite{Duala-Ekoko:2012:AAQ:2337223.2337255}. They categorized the API related information needs into twenty generic questions that can be used to analyze the quality of API documentation.

  In this paper, we focused on the usage examples part of API documentation to further understand the obstacles that API client developers face when the documentation contains relevant information about the API, it's intent, description of the API actions and elements, but lacks code examples.

\subsubsection{Crowd-Sourced API Usage Examples}
Several existing research has focused on sourcing API usage examples from the Internet. Wang et al. performed an exploratory study to understand the current state of API related knowledge available on the Internet that can be leveraged by API client developers \cite{6462686}. They searched the web for usage examples related to the API of five popular Java libraries and found that on an average API examples could be found for 77\% of the 4,637 APIs included in the selected libraries. Moreover, they found that the crowd-sourced API documented accounted for 93.7\% of the usage examples compared to 6.13\% that were published on the official documentation. Incorporating a corpus of crowd-sourced API examples from the web and using placeholders for commonly related API elements, an evaluation of Jadite found that developers were three times faster to complete API tasks \cite{5295283}. Nasehi et al. analyzed StackOverflow threads to understand the characteristics of good and bad code examples based on user provided votes \cite{Nasehi_what_makes}. They found that commonly down-voted API related answers lacked code examples, explanations, and shortcoming of solutions. They recommended API developers to evolve the documentation with usage examples to answer API client developer questions that were not anticipated in the existing documentation. Treude et al. proposed a machine learning based approach to find relevant API documentation from StackOverflow using both the text and metadata found from the questions and answers \cite{Treude:2016:AAD:2884781.2884800}. Kim et al. presented a technique to automatically augment code examples from the web using code search tools to Java APIs \cite{Kim:2009:AEJ:1747491.1747552}. Jiau et al. observed a severe inequality within the context of crowd-sourced API usage examples where most of the content were related to the popular APIs \cite{jiau}.

Existing research on crowd-sourced API documentation has mostly focused finding crowd-sourced usage examples of local APIs. In this paper, we focused on the impact of usage examples for REST APIs, where the API client development programming language is agnostic to the language used to implement the API. This important distinction with local APIs makes it hard to leverage the techniques proposed in the aforementioned papers to find relevant REST API usage examples.

\subsubsection{Measuring API Usability}
Several papers have been published on the topic of measuring API usability. Rama et al. presented a set of formulas for computing a measure of API usability based on the API's structural components such as classes, methods, parameters, return values, thread-safety, etc \cite{rama}. Scheller et al. presented a framework for automatically measuring the complexity of an API \cite{scheller2015automated}. They identified a list of measurable API properties and provided formulas to compute complexities related to the interfaces, implementation and setup of an API. Grill et al. proposed an HCI based approach that can be used by API developers to understand and improve on the problem areas related to an API \cite{Grill2012}. They suggested using a combination of expert opinion and developer workshops on APIs to identify and collect feedback about API and its documentation related problems. In our work, we've focused on the usability related properties of an API and proved relationship between API client developer success and the presence of usage examples on API documentation.

\subsubsection{Controlled Studies}
Several authors published the results of controlled studies on the topic of API usability. Nasehi et al. performed a controlled study to understand if API unit tests can be used to provide as usage examples to facilitate API client developers \cite{5609553}. They grouped participants into two groups, and one of the groups was provided with API unit tests in addition to the documentation. The two groups performed the same set of API tasks. The researchers found that the examples from the unit tests helped understanding the API concepts better but it was challenging for the participants to locate relevant examples from the corpus of unit tests. They recommended automated extraction of relevant high-level API usage examples from API unit tests. Endrikat performed a controlled study with four groups of participants to understand the impact of API documentation on APIs that are implemented using programming languages with static and dynamic type systems \cite{Endrikat:2014:ADS:2568225.2568299}. Participants were given a set of failing unit tests and were asked to make them pass by writing code using the studied API. They defined the API client developer productivity in terms of the time to get the tests passing. They found that the participant group using a statically typed API with explicit documentation were more productive than the other groups. Ellis et al. performed a controlled study with two groups of Java developers to understand the impact of using a constructor vs. a factory method design pattern on API client development time \cite{Ellis:2007:FPA:1248820.1248863}. They found that API client developers needed more time to complete API tasks when the API requires the use a factory method to instantiate objects compared to using a constructor.

Our work in this paper is based on a controlled study and shares part of the setup that were used by the aforementioned API related controlled studies with the following differences: our goal is to understand the obstacles faced by API client developers without usage examples;  we focus on REST APIs instead of a local API; and, the participants are professional software engineers.

\section{Conclusion}
\section*{Acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,refs}


\end{document}


